{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        tensor = torch.from_numpy(np.array(sample[\"image\"]/255).astype(np.float32))\n",
    "        target = torch.from_numpy(np.array(sample[\"target\"]).astype(np.float32))\n",
    "        \n",
    "        return {\"target\":target, \"image\":tensor}\n",
    "\n",
    "class mnist_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, transform = None):\n",
    "        \n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.target = pd.get_dummies(test[\"label\"]).values\n",
    "        self.variable = df.drop(columns=[\"label\"]).values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = {'target':self.target[idx],'image':self.variable[idx]}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load dataset\n",
    "mnist_df = mnist_dataset(\"train.csv\")\n",
    "\n",
    "#plot some images\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    idx = random.randint(0, len(mnist_df)-1)\n",
    "    plt.imshow(mnist_df[idx][\"image\"].reshape(28,28))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 layer Neural Network\n",
    "\n",
    "batch_size = 32\n",
    "input_size = len(mnist_df[0][\"image\"])\n",
    "output_layer_size = len(mnist_df[0][\"target\"])\n",
    "hidden_layer_size = 100\n",
    "\n",
    "mnist_df = mnist_dataset(\"train.csv\", transform = (ToTensor()))\n",
    "dataset_loader = torch.utils.data.DataLoader(mnist_df, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#intialize weights\n",
    "w1 = torch.randn(input_size, hidden_layer_size, dtype = torch.float)\n",
    "b1 = torch.randn(hidden_layer_size)\n",
    "w2 = torch.randn(hidden_layer_size, output_layer_size, dtype = torch.float)\n",
    "b2 = torch.zeros(output_layer_size)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, cost:-384.6148681640625\n",
      "iteration: 1, cost:-350.4373779296875\n",
      "iteration: 2, cost:-357.7351989746094\n",
      "iteration: 3, cost:-323.8299255371094\n",
      "iteration: 4, cost:-350.21905517578125\n",
      "iteration: 5, cost:-397.6826171875\n",
      "iteration: 6, cost:-393.8050842285156\n",
      "iteration: 7, cost:-442.5880432128906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(400):\n",
    "    for idx, batch in enumerate(dataset_loader, 1):\n",
    "\n",
    "        X = batch[\"image\"]\n",
    "        Y = batch[\"target\"]\n",
    "\n",
    "        #forward propagation\n",
    "        Z1 = X.mm(w1) + b1\n",
    "        X2 = Z1.tanh()\n",
    "        Z2 = X2.mm(w2) + b2\n",
    "        A = torch.sigmoid(z2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #backward propagation\n",
    "        dZ2 = A - Y\n",
    "        dW2 = dZ2.t().mm(X2).t() * 1/m\n",
    "        db2 = torch.sum(dZ2, dim=0)\n",
    "        dZ1 = torch.mul(dZ2.mm(w2.t()), 1-X2.pow(2))\n",
    "        dW1 = X.t().mm(dZ1) * 1/m\n",
    "        db1 = dZ1.sum(dim=0)\n",
    "        \n",
    "        w1 = w1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "        w2 = w2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        \n",
    "    #Cost / Loss\n",
    "    cost = (torch.sum(torch.mul(torch.log(A), Y) + torch.mul((1-Y), torch.log(1-A))) ) / len(A)\n",
    "    print(\"iteration: {}, cost:{}\".format(i, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 100])"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.t().mm(torch.mul(dZ2.mm(w2.t()), 1-X2.pow(2))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mnist_df[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mnist_df[0][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "z1 = x.mm(w1) + b1\n",
    "x2 = z1.tanh()\n",
    "z2 = x2.mm(w2) + b2\n",
    "A = torch.sigmoid(z2)\n",
    "\n",
    "# backward propagation\n",
    "\n",
    "dZ2 = A - Y\n",
    "dW2 = dZ2.t().mm(x2).t() * 1/m\n",
    "db2 = torch.sum(dZ2, dim=0)\n",
    "dZ1 = torch.mul(dZ2.mm(w2.t()), 1-z1.pow(2))\n",
    "dW1 = x.t().mm(dZ1) * 1/m\n",
    "db1 = dZ1.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = (torch.sum(torch.mul(torch.log(A), Y) + torch.mul((1-Y), torch.log(1-A))) ) / len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1.size(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 100])"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=dZ2.t().mm(x2).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=g.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7607,  2.9369,  0.5967, -0.0079, -1.4002,  0.0081, -0.9959, -0.3968,\n",
       "         -1.5415, -0.9113],\n",
       "        [-0.0557, -1.2771,  1.7322, -0.4332,  0.3828,  0.7154, -0.0916,  1.1509,\n",
       "         -0.1248,  0.1637],\n",
       "        [-0.4942,  2.7576, -0.5755, -0.6501, -0.8592, -1.0933,  1.8032, -2.3724,\n",
       "         -1.1119,  0.2740],\n",
       "        [-0.6346,  0.5143,  0.7830, -1.1630,  0.5640, -0.8237, -1.2244,  0.5636,\n",
       "          0.0202,  0.1795],\n",
       "        [ 1.0715, -0.7036,  0.6225,  0.3146,  0.6557, -0.6905,  1.0418,  0.4318,\n",
       "          0.4224, -0.6583],\n",
       "        [-1.1561,  1.8455,  0.1888, -0.2077,  0.9009, -0.7983, -1.5306, -0.7259,\n",
       "          1.0418, -0.9802],\n",
       "        [-0.9347,  0.2285,  0.3882,  1.2153, -0.5829,  2.3629,  1.7447, -0.1498,\n",
       "          0.5409, -0.8059],\n",
       "        [ 1.4517,  0.6645,  0.4231, -1.1929, -0.0321, -0.1120, -0.3116, -0.9900,\n",
       "         -1.3029, -1.0502],\n",
       "        [ 0.2264,  1.6344, -0.7792,  0.0116, -2.0122,  1.0926, -1.7063, -2.5952,\n",
       "         -0.5151,  0.4394],\n",
       "        [ 0.0440, -1.2109,  1.1017,  0.9727,  0.1700,  0.7345, -0.0061,  1.9303,\n",
       "          0.4038,  0.2108],\n",
       "        [ 0.0468,  0.1840, -1.9724,  2.5544,  1.4575, -1.0359, -0.8399, -0.8015,\n",
       "         -2.8957, -0.6401],\n",
       "        [-0.7834,  0.3576,  0.9010,  0.0134,  0.9523, -0.8600, -1.1901, -1.6951,\n",
       "         -0.5871, -1.0571],\n",
       "        [ 0.7513, -1.0666, -1.2827, -1.9718, -1.0128,  2.4900,  0.0085, -1.4019,\n",
       "          0.8655, -0.6716],\n",
       "        [ 1.4948, -2.7881,  0.4481,  0.4191,  0.1657,  1.5514,  1.0539, -1.2667,\n",
       "          0.1077,  0.1846],\n",
       "        [ 1.2758, -0.7409, -0.8011,  0.1095,  0.4630,  1.2513, -1.5627,  1.0318,\n",
       "          0.5165, -0.9167],\n",
       "        [ 0.3697, -1.0444,  0.5831, -0.0048,  1.3332, -0.7829, -1.4048,  2.8710,\n",
       "         -0.6755, -1.4232],\n",
       "        [-1.2975, -0.0300, -2.3408,  1.6465,  0.0694, -0.8116,  0.3136, -0.6823,\n",
       "         -0.6686, -0.8093],\n",
       "        [-0.9359, -0.8979, -0.4131, -0.8108,  0.1326, -0.2831, -0.2496, -4.2076,\n",
       "         -1.3350,  0.2008],\n",
       "        [ 1.4273, -0.0181, -0.6593,  0.2170,  1.6312,  2.5408,  0.2014, -0.8976,\n",
       "          2.0400, -0.3218],\n",
       "        [ 1.6259,  1.8641, -0.8335, -0.3723, -0.1405, -1.8495, -1.7263, -0.0152,\n",
       "         -0.8390, -0.7501],\n",
       "        [-1.9575,  1.0900,  1.2498,  0.5521,  0.3286, -0.6075, -0.2150,  2.8419,\n",
       "          1.3398,  1.3948],\n",
       "        [-0.0545, -2.2945,  0.8919,  0.5669,  1.5132,  1.4502, -0.5946,  2.2542,\n",
       "          0.8864,  0.2523],\n",
       "        [ 0.7614, -2.1244,  1.1105,  1.5488, -0.3930,  0.8574,  2.5561,  1.9991,\n",
       "          0.7364, -0.1097],\n",
       "        [ 0.4274, -0.8325,  0.2743,  1.6162, -1.4408,  1.2783,  0.5156, -1.4635,\n",
       "          0.2652,  0.4541],\n",
       "        [ 0.1139,  0.9805, -1.1125, -1.7244,  1.7633, -2.3751,  0.6210, -0.3973,\n",
       "         -0.4753, -0.8268],\n",
       "        [-0.7897,  2.0769, -0.6361, -0.7458, -2.1807, -0.4129,  0.6276, -1.2963,\n",
       "         -0.6703,  0.3755],\n",
       "        [ 0.4250, -2.7472, -0.7156, -0.6912,  2.5229,  0.6422,  0.6237, -0.4699,\n",
       "          1.2687, -1.6671],\n",
       "        [ 0.4379,  0.1384, -0.0451,  0.2841,  1.3472,  0.2314, -1.0483,  0.6956,\n",
       "          0.6914,  0.4482],\n",
       "        [-0.4040,  0.0184, -1.1100,  0.8868,  0.3337,  1.1733,  0.2669,  1.3170,\n",
       "          1.9401,  0.6952],\n",
       "        [ 0.7280,  1.5379,  0.0440,  0.0698, -0.5195,  1.8184, -1.1810, -1.8623,\n",
       "         -2.1525,  1.0080],\n",
       "        [ 1.7222, -0.7262,  0.2993, -0.0846, -0.5621, -1.9123,  1.6226,  1.2748,\n",
       "          0.0330,  0.7730],\n",
       "        [-0.4761, -0.4414,  0.5935, -1.6583, -0.5039,  1.5262, -0.0045,  0.0940,\n",
       "          1.3009, -0.0877],\n",
       "        [-0.4425,  0.1220, -0.8463,  2.1288,  0.5858, -1.0428, -1.1922, -1.8843,\n",
       "         -1.7073, -0.5325],\n",
       "        [ 0.3557, -0.2675,  0.9348, -0.3058, -2.6971,  0.6649, -0.2192,  0.7628,\n",
       "          0.0536, -0.5611],\n",
       "        [-0.0526, -0.5950,  0.3751, -0.0398,  0.4922,  0.6878, -0.5316,  0.0708,\n",
       "         -0.6777,  1.0631],\n",
       "        [ 2.0132, -0.8206,  0.3967, -0.1050,  0.7276,  1.6409,  1.2839,  1.1666,\n",
       "          0.4122,  0.8270],\n",
       "        [-0.1176,  1.1119, -0.3922, -0.9187,  0.3435,  0.8179,  0.2761, -0.8433,\n",
       "          0.2795, -1.1140],\n",
       "        [ 0.5851, -0.2469, -2.4253, -0.4119, -0.9090,  0.1761, -1.3867,  1.9323,\n",
       "          1.4308,  0.9526],\n",
       "        [ 1.0578, -0.2359, -1.6404,  1.7240, -1.2854, -0.2377, -0.4912, -1.4945,\n",
       "         -1.5956, -0.0124],\n",
       "        [-0.0415, -0.6936,  0.8180, -0.3133,  0.0560, -0.2828,  0.5663, -0.3759,\n",
       "         -0.1341, -0.8285],\n",
       "        [-0.5741,  0.6548, -0.4006, -0.9978,  0.0258,  0.3998, -1.6554, -0.5606,\n",
       "          0.4559, -0.7794],\n",
       "        [-1.5921, -1.6602,  1.6060,  0.6513, -0.0224,  2.6974, -1.5416,  0.1167,\n",
       "          0.4231,  0.7418],\n",
       "        [ 0.6125,  1.7018,  1.1242, -1.1372,  0.6346, -0.3741, -0.4032, -2.3227,\n",
       "         -0.1604, -0.5045],\n",
       "        [ 1.0575,  2.3971, -0.0666, -0.8086, -0.7705, -1.7467,  1.0081, -1.1627,\n",
       "         -0.5967, -0.5770],\n",
       "        [-0.9057,  0.9723,  1.5308, -0.1487,  0.8653, -0.7321, -1.4225, -1.3284,\n",
       "         -0.1512,  0.2580],\n",
       "        [ 0.1243, -1.6327,  0.7173, -0.9331,  2.4646, -1.0828, -0.4073, -0.1457,\n",
       "          0.9602,  0.2387],\n",
       "        [-0.9312, -0.7836, -2.0677, -1.2192, -0.9725,  1.1848, -1.3906,  0.0111,\n",
       "          0.0488, -0.9311],\n",
       "        [ 1.3500,  0.2330, -0.9320, -1.0625,  1.3772, -1.1850, -0.7551, -2.5732,\n",
       "         -2.4413, -0.3030],\n",
       "        [-2.6493,  1.2687, -0.0355, -2.4452,  0.3037, -0.5736, -0.2040, -0.9949,\n",
       "         -2.5311, -1.2576],\n",
       "        [-1.1582, -0.4935, -1.2101,  0.3064, -0.6037, -2.1503, -0.7930, -1.5704,\n",
       "          0.4222,  0.0452],\n",
       "        [-0.0681, -0.7760,  0.5131,  0.8805,  0.1750,  2.2410,  1.0302,  0.0936,\n",
       "          1.9995,  0.8587],\n",
       "        [ 0.0163, -1.3445,  0.6163, -1.7104,  0.7555, -0.8171,  1.6526,  0.4111,\n",
       "          0.2075, -0.6697],\n",
       "        [ 0.7840, -1.2089,  0.1907, -0.3427,  0.8371,  1.8010,  2.6009,  1.2622,\n",
       "          0.5218,  0.3313],\n",
       "        [-1.6086,  1.2773, -0.2624,  0.4998,  0.2678, -0.5619, -0.7123, -1.6937,\n",
       "         -1.6914, -1.3070],\n",
       "        [ 0.0104,  0.4497,  0.9172,  0.9387, -1.6240,  0.6169, -1.2242, -1.0784,\n",
       "          0.5229, -1.7274],\n",
       "        [ 0.0559,  1.1516, -0.1706, -0.1185,  1.5868, -1.4195, -0.6810, -0.5823,\n",
       "         -0.5996,  0.3605],\n",
       "        [ 3.0192, -0.4056, -0.6744, -0.0406, -0.4899,  0.9426, -0.6651, -0.2101,\n",
       "         -1.1466, -1.6123],\n",
       "        [-0.0477,  1.9206, -0.2442, -0.7515, -0.9303,  0.3577, -1.1947, -3.2062,\n",
       "         -0.2906,  0.1570],\n",
       "        [-0.0260,  0.2169,  1.7279, -0.0687, -1.2370, -1.7567, -0.6287,  0.2019,\n",
       "         -1.3529,  0.1467],\n",
       "        [ 1.2410,  0.1442, -0.8600,  1.7388,  0.4573,  0.4564, -1.5430,  0.6569,\n",
       "          0.0145,  1.4951],\n",
       "        [ 0.5941, -0.2837, -0.2235,  1.6798, -0.5679,  0.8408, -0.0562,  0.5365,\n",
       "          0.5090,  1.1045],\n",
       "        [-0.6765,  1.3094, -1.7626,  0.2044,  1.0474, -2.3678, -1.3023, -2.4470,\n",
       "          0.0360,  0.0602],\n",
       "        [ 1.0032,  0.6413, -0.4258, -0.3017, -1.1426, -1.5602, -1.7800,  1.2515,\n",
       "          0.3432,  0.0241],\n",
       "        [ 0.7001, -0.2152,  0.2271, -0.7678, -0.8441, -2.2491, -0.2847, -0.1051,\n",
       "         -0.2211,  0.9071],\n",
       "        [-2.7398, -1.9176, -0.4651,  0.3677,  1.0653,  1.1918, -0.4817, -0.6322,\n",
       "          0.3648,  0.3736],\n",
       "        [ 0.6409, -0.5566, -0.3316, -0.1748, -0.5875, -0.4246, -1.2054,  0.0142,\n",
       "          1.4240, -0.2363],\n",
       "        [-0.0512,  2.5573,  0.9448,  0.7632, -0.2580, -1.6580, -1.1678,  0.9447,\n",
       "         -0.7803,  1.4499],\n",
       "        [ 0.3667,  3.1244, -1.2058,  0.6870, -3.5090, -0.1052, -0.1581,  0.6658,\n",
       "         -0.7529,  0.1538],\n",
       "        [-1.1353, -0.3300,  1.1476,  0.5933,  0.3522, -2.5580, -0.3846, -1.5673,\n",
       "         -1.0153,  0.5293],\n",
       "        [-1.8941, -2.2696,  0.5591, -0.3972, -0.1555,  3.2415,  1.5507,  0.7172,\n",
       "          2.2759,  0.0830],\n",
       "        [ 1.5660, -2.2315, -0.6593,  0.7955,  0.5806,  0.7577,  0.9090,  1.5399,\n",
       "          0.8729,  1.3865],\n",
       "        [ 0.2950, -0.3223,  0.0360,  1.4024, -0.2794,  0.6911,  1.3573,  0.8945,\n",
       "          0.4418, -1.0830],\n",
       "        [-0.0889,  2.3380,  0.8355, -0.6030, -0.0739,  0.5811, -0.1056, -2.6000,\n",
       "          1.1174,  1.1590],\n",
       "        [ 0.1831, -0.6806, -0.0568, -1.6341,  1.0794,  1.0503,  0.1996, -2.0037,\n",
       "         -2.0898, -0.6537],\n",
       "        [-1.3259, -0.9026, -0.4798,  0.6670,  0.1731,  0.7270,  1.7496, -1.3071,\n",
       "         -0.9575,  1.4764],\n",
       "        [ 0.5812,  1.1170,  0.3433, -1.2133,  0.4414, -1.3519, -1.0828, -1.4092,\n",
       "         -0.6198, -1.3877],\n",
       "        [ 0.4448,  1.5035, -0.0178,  0.2817,  0.7636, -2.4590, -0.9050,  0.1293,\n",
       "          0.0566, -0.1180],\n",
       "        [ 0.8497,  0.7310,  0.2340,  1.8382,  0.0222, -1.5696,  0.8519, -1.2004,\n",
       "         -2.7426, -0.2795],\n",
       "        [-0.2004, -0.3628, -0.4031,  0.1306,  0.7652, -0.2347,  0.8798,  0.4751,\n",
       "          1.5153, -0.3441],\n",
       "        [ 0.2038, -1.4050, -0.1150,  1.7440, -1.3959,  1.1245,  1.0730, -0.1915,\n",
       "         -0.1990, -0.5183],\n",
       "        [-0.3905,  1.5546, -0.2100,  1.6046, -0.0397, -2.4222,  1.9738, -0.4400,\n",
       "         -1.6487, -1.4119],\n",
       "        [ 0.2608,  1.7128,  1.9362,  0.8687, -0.6961,  0.4584,  0.8476,  0.1813,\n",
       "          0.2251, -0.4608],\n",
       "        [ 0.4846, -0.0410, -1.5830,  0.0135, -0.1321, -2.1328, -1.5886, -0.8651,\n",
       "         -1.5749, -2.7676],\n",
       "        [ 2.1517,  0.5567, -1.0154,  0.8236, -0.8508, -0.0693, -0.8087, -1.4788,\n",
       "         -0.5344,  0.3972],\n",
       "        [ 1.9833, -1.7575,  0.3358,  0.6892, -2.6194,  1.4965,  0.2342,  0.3480,\n",
       "         -1.0494,  0.4734],\n",
       "        [ 0.4103, -0.6980, -1.1065,  0.5752,  1.2129,  0.6151,  1.3368,  1.5249,\n",
       "          1.1902, -0.0501],\n",
       "        [-0.8670, -1.6654, -0.4882, -2.3176,  0.5307,  1.2401,  0.9653,  0.6120,\n",
       "          1.4446, -0.4965],\n",
       "        [-0.0815, -0.2548, -0.8982, -0.4013,  0.4714,  0.3902,  0.7476,  0.8275,\n",
       "          0.2601, -1.1265],\n",
       "        [-0.5298, -1.9042,  2.1410, -0.0479,  0.4993,  0.8462, -0.7863,  1.4152,\n",
       "          2.4070, -2.3731],\n",
       "        [ 0.7849,  0.9914,  1.5410, -1.1665,  0.6319, -1.6694, -1.5581, -0.3716,\n",
       "         -0.7505, -2.2031],\n",
       "        [-1.0839,  1.0523, -0.1372,  0.6651,  0.2487,  0.5093, -0.6393, -0.1816,\n",
       "          1.8483, -0.3243],\n",
       "        [-1.6418, -0.7427, -1.0407, -0.2243,  0.0920, -2.3790,  0.3554, -1.2267,\n",
       "         -1.7181,  0.7778],\n",
       "        [ 1.3551, -2.0132, -3.0929,  0.4367,  1.6199,  0.8709,  1.2372, -0.9752,\n",
       "          1.0877,  1.5680],\n",
       "        [ 0.4400, -1.2258, -2.4164,  0.0677,  0.0487,  1.2269,  2.1399,  2.2515,\n",
       "          1.4586,  0.8171],\n",
       "        [-0.8926, -0.2788,  0.7366, -0.7237, -0.0979,  0.5219, -0.3869,  2.7403,\n",
       "          0.9940, -0.1276],\n",
       "        [ 0.2150,  0.3801, -0.7769, -0.3760, -0.7274, -1.3020, -0.4415, -2.1102,\n",
       "         -0.6830, -2.3696],\n",
       "        [-1.4916, -0.9667,  1.2042,  1.9614, -0.2566, -0.3287, -0.3875,  1.4082,\n",
       "         -0.1896,  0.5373],\n",
       "        [-1.1307,  0.4316,  0.6129, -0.2880, -0.7960,  1.9941,  0.8444,  0.4730,\n",
       "         -0.6774, -1.2005],\n",
       "        [ 0.7139, -1.6526,  0.2696,  0.1097, -0.2052,  2.5938, -0.3547,  1.9956,\n",
       "          0.2988,  1.2818],\n",
       "        [ 0.2625,  0.8227,  0.6822, -0.3003, -0.3941, -1.4816, -1.0681,  0.2764,\n",
       "         -0.6447,  0.4865]])"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 - g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_input = np.dot(dZ2,w2.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-541-7254ab75312a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdZ2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "w1 - dZ2.t().mm(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def backward(self,input,grad_output):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output,np.transpose(self.weights))\n",
    "\n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.transpose(np.dot(np.transpose(grad_output),input))\n",
    "        grad_biases = np.sum(grad_output, axis = 0)\n",
    "        \n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        # Later on, you can try replacing that with something better.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'Transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-354-65545e3d202c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTranspose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'Transpose'"
     ]
    }
   ],
   "source": [
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        grad_weights = np.transpose(np.dot(np.transpose(grad_output),input))\n",
    "        grad_biases = np.sum(grad_output, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0084e-04, -9.8701e-01,  5.6031e-03,  1.9205e-05,  1.0389e-06,\n",
       "          1.0000e+00,  3.7445e-01,  9.8186e-01,  8.1262e-01,  8.7171e-10]])"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.0084e-05, -9.8701e-02,  5.6031e-04,  1.9205e-06,  1.0389e-07,\n",
       "         1.0000e-01,  3.7445e-02,  9.8186e-02,  8.1262e-02,  8.7171e-11])"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " * 1/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-474-c91afc8b1b78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdZ2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdW2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: dot: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "dZ2 = A - Y\n",
    "dW2 = torch.dot(dZ2, A.t()) * 1/m\n",
    "db2 = (1 / m) * dZ2.sum()\n",
    "dZ1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = torch.sum(dZ2) * 1/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-387-759cda7eba02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdZ2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: dot: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "dZ2.dot(w2.t()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(np.dot(dZ2, w2.t()), A.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-384-759cda7eba02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdZ2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: dot: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "dZ2.dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [10 x 100], m2: [1 x 10] at c:\\a\\w\\1\\s\\tmp_conda_3.7_104535\\conda\\conda-bld\\pytorch_1550400486030\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-416-dbd2b96dfca4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [10 x 100], m2: [1 x 10] at c:\\a\\w\\1\\s\\tmp_conda_3.7_104535\\conda\\conda-bld\\pytorch_1550400486030\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 9.9983e-01, 9.9997e-01, 1.0000e+00, 1.0000e+00, 5.0068e-06,\n",
       "         8.5979e-01, 3.5942e-02, 3.3965e-01, 1.0000e+00]])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,100) (1,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-465-7442dc0b6dc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,100) (1,10) "
     ]
    }
   ],
   "source": [
    "np.multiply(np.dot(dZ2, w2.t()), 1 - np.power(A, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,100) and (1,10) not aligned: 100 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-470-86a44164ef91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdZ2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,100) and (1,10) not aligned: 100 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(w2.t(), dZ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 1])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 100])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [10 x 100], m2: [1 x 10] at c:\\a\\w\\1\\s\\tmp_conda_3.7_104535\\conda\\conda-bld\\pytorch_1550400486030\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-429-dbd2b96dfca4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [10 x 100], m2: [1 x 10] at c:\\a\\w\\1\\s\\tmp_conda_3.7_104535\\conda\\conda-bld\\pytorch_1550400486030\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940"
     ]
    }
   ],
   "source": [
    "w2.t().mm(dZ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ2 = A - Y\n",
    "dW2 = (1/len(A)) * torch.mm(dZ2, torch.tr)\n",
    "    \n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2)\n",
    "cost = - np.sum(logprobs) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.7142e-01, 9.8408e-01, 4.4570e-01, 1.1091e-02, 9.9317e-01, 4.5503e-01,\n",
       "         4.1570e-05, 2.2966e-06, 9.4397e-01, 9.9969e-01]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "y_pred = F.relu(x.mm(w1)).mm(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 100])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34189644.0\n",
      "1 29711042.0\n",
      "2 25080740.0\n",
      "3 18660142.0\n",
      "4 12330950.0\n",
      "5 7497069.0\n",
      "6 4546526.0\n",
      "7 2902201.25\n",
      "8 2006440.75\n",
      "9 1492909.875\n",
      "10 1173900.875\n",
      "11 957528.6875\n",
      "12 799244.8125\n",
      "13 677228.875\n",
      "14 579924.8125\n",
      "15 500347.90625\n",
      "16 434364.65625\n",
      "17 379013.25\n",
      "18 332180.4375\n",
      "19 292356.6875\n",
      "20 258268.796875\n",
      "21 228920.875\n",
      "22 203582.75\n",
      "23 181601.8125\n",
      "24 162425.640625\n",
      "25 145649.0625\n",
      "26 130930.4453125\n",
      "27 117981.171875\n",
      "28 106551.515625\n",
      "29 96440.2890625\n",
      "30 87453.6875\n",
      "31 79450.4375\n",
      "32 72303.5390625\n",
      "33 65906.796875\n",
      "34 60172.09375\n",
      "35 55019.1796875\n",
      "36 50379.0234375\n",
      "37 46193.25\n",
      "38 42412.546875\n",
      "39 38990.953125\n",
      "40 35888.5859375\n",
      "41 33071.70703125\n",
      "42 30509.693359375\n",
      "43 28177.359375\n",
      "44 26056.974609375\n",
      "45 24121.87109375\n",
      "46 22351.509765625\n",
      "47 20729.916015625\n",
      "48 19243.84765625\n",
      "49 17879.345703125\n",
      "50 16626.712890625\n",
      "51 15475.47265625\n",
      "52 14414.658203125\n",
      "53 13436.5048828125\n",
      "54 12533.4931640625\n",
      "55 11699.3125\n",
      "56 10928.251953125\n",
      "57 10214.3896484375\n",
      "58 9553.46875\n",
      "59 8940.7294921875\n",
      "60 8371.9814453125\n",
      "61 7843.9423828125\n",
      "62 7353.1083984375\n",
      "63 6896.76416015625\n",
      "64 6472.0390625\n",
      "65 6076.7578125\n",
      "66 5708.359375\n",
      "67 5364.94189453125\n",
      "68 5044.3974609375\n",
      "69 4745.1396484375\n",
      "70 4465.650390625\n",
      "71 4204.51025390625\n",
      "72 3960.226806640625\n",
      "73 3731.54345703125\n",
      "74 3517.3232421875\n",
      "75 3316.780517578125\n",
      "76 3128.777587890625\n",
      "77 2952.497314453125\n",
      "78 2787.240966796875\n",
      "79 2632.134521484375\n",
      "80 2486.445556640625\n",
      "81 2349.6083984375\n",
      "82 2220.99853515625\n",
      "83 2100.126953125\n",
      "84 1986.477783203125\n",
      "85 1879.52001953125\n",
      "86 1778.85205078125\n",
      "87 1684.03369140625\n",
      "88 1594.7559814453125\n",
      "89 1510.616943359375\n",
      "90 1431.3271484375\n",
      "91 1356.5469970703125\n",
      "92 1286.0108642578125\n",
      "93 1219.4510498046875\n",
      "94 1156.6087646484375\n",
      "95 1097.274658203125\n",
      "96 1041.228271484375\n",
      "97 988.2658081054688\n",
      "98 938.1917724609375\n",
      "99 890.8614501953125\n",
      "100 846.1303100585938\n",
      "101 803.79736328125\n",
      "102 763.72509765625\n",
      "103 725.7911987304688\n",
      "104 689.8935546875\n",
      "105 655.9143676757812\n",
      "106 623.709716796875\n",
      "107 593.2122192382812\n",
      "108 564.3031005859375\n",
      "109 536.9000244140625\n",
      "110 510.9093017578125\n",
      "111 486.2614440917969\n",
      "112 462.8875427246094\n",
      "113 440.7030029296875\n",
      "114 419.6595458984375\n",
      "115 399.67376708984375\n",
      "116 380.71063232421875\n",
      "117 362.6993103027344\n",
      "118 345.580810546875\n",
      "119 329.3226013183594\n",
      "120 313.87725830078125\n",
      "121 299.19500732421875\n",
      "122 285.2412109375\n",
      "123 271.97613525390625\n",
      "124 259.357177734375\n",
      "125 247.35897827148438\n",
      "126 235.95352172851562\n",
      "127 225.09669494628906\n",
      "128 214.7640380859375\n",
      "129 204.9366455078125\n",
      "130 195.5785675048828\n",
      "131 186.66827392578125\n",
      "132 178.18898010253906\n",
      "133 170.109130859375\n",
      "134 162.4168243408203\n",
      "135 155.09259033203125\n",
      "136 148.11154174804688\n",
      "137 141.45994567871094\n",
      "138 135.12066650390625\n",
      "139 129.07876586914062\n",
      "140 123.3218994140625\n",
      "141 117.82984924316406\n",
      "142 112.59391784667969\n",
      "143 107.60272979736328\n",
      "144 102.84452819824219\n",
      "145 98.30551147460938\n",
      "146 93.97576904296875\n",
      "147 89.84371948242188\n",
      "148 85.90249633789062\n",
      "149 82.13951873779297\n",
      "150 78.55042266845703\n",
      "151 75.12311553955078\n",
      "152 71.8533935546875\n",
      "153 68.73135375976562\n",
      "154 65.75033569335938\n",
      "155 62.90406799316406\n",
      "156 60.18311309814453\n",
      "157 57.586483001708984\n",
      "158 55.104698181152344\n",
      "159 52.73581314086914\n",
      "160 50.47272491455078\n",
      "161 48.31085205078125\n",
      "162 46.24604034423828\n",
      "163 44.27210235595703\n",
      "164 42.385677337646484\n",
      "165 40.58192825317383\n",
      "166 38.85784912109375\n",
      "167 37.20945358276367\n",
      "168 35.633907318115234\n",
      "169 34.12626266479492\n",
      "170 32.686180114746094\n",
      "171 31.30869483947754\n",
      "172 29.99111557006836\n",
      "173 28.7301025390625\n",
      "174 27.52462387084961\n",
      "175 26.370967864990234\n",
      "176 25.267078399658203\n",
      "177 24.21082305908203\n",
      "178 23.200420379638672\n",
      "179 22.233112335205078\n",
      "180 21.308040618896484\n",
      "181 20.422136306762695\n",
      "182 19.57430648803711\n",
      "183 18.76288414001465\n",
      "184 17.985797882080078\n",
      "185 17.242385864257812\n",
      "186 16.5294246673584\n",
      "187 15.848262786865234\n",
      "188 15.195115089416504\n",
      "189 14.570036888122559\n",
      "190 13.97114372253418\n",
      "191 13.39768123626709\n",
      "192 12.848125457763672\n",
      "193 12.322226524353027\n",
      "194 11.818154335021973\n",
      "195 11.335731506347656\n",
      "196 10.873106002807617\n",
      "197 10.429537773132324\n",
      "198 10.004870414733887\n",
      "199 9.598058700561523\n",
      "200 9.208283424377441\n",
      "201 8.834415435791016\n",
      "202 8.476530075073242\n",
      "203 8.133195877075195\n",
      "204 7.804384231567383\n",
      "205 7.488985061645508\n",
      "206 7.186644554138184\n",
      "207 6.896878242492676\n",
      "208 6.618899345397949\n",
      "209 6.352554798126221\n",
      "210 6.097322940826416\n",
      "211 5.852179050445557\n",
      "212 5.617541790008545\n",
      "213 5.392457962036133\n",
      "214 5.176570415496826\n",
      "215 4.9692769050598145\n",
      "216 4.770814418792725\n",
      "217 4.580169200897217\n",
      "218 4.397525787353516\n",
      "219 4.222474575042725\n",
      "220 4.054134368896484\n",
      "221 3.8928802013397217\n",
      "222 3.7382285594940186\n",
      "223 3.589604616165161\n",
      "224 3.4469966888427734\n",
      "225 3.310147523880005\n",
      "226 3.1788887977600098\n",
      "227 3.053071975708008\n",
      "228 2.932295322418213\n",
      "229 2.8163211345672607\n",
      "230 2.704984664916992\n",
      "231 2.598242998123169\n",
      "232 2.495697021484375\n",
      "233 2.397348403930664\n",
      "234 2.3031182289123535\n",
      "235 2.2124903202056885\n",
      "236 2.1253931522369385\n",
      "237 2.041985034942627\n",
      "238 1.9618057012557983\n",
      "239 1.8849241733551025\n",
      "240 1.811052680015564\n",
      "241 1.7401398420333862\n",
      "242 1.672160267829895\n",
      "243 1.6067087650299072\n",
      "244 1.5440329313278198\n",
      "245 1.483638048171997\n",
      "246 1.4257616996765137\n",
      "247 1.3702051639556885\n",
      "248 1.3168785572052002\n",
      "249 1.2657358646392822\n",
      "250 1.216500997543335\n",
      "251 1.1692482233047485\n",
      "252 1.123849630355835\n",
      "253 1.0803232192993164\n",
      "254 1.0383387804031372\n",
      "255 0.9981563687324524\n",
      "256 0.9595962166786194\n",
      "257 0.9223719239234924\n",
      "258 0.8867444396018982\n",
      "259 0.8525077104568481\n",
      "260 0.8196097016334534\n",
      "261 0.7879178524017334\n",
      "262 0.7576474547386169\n",
      "263 0.7282971739768982\n",
      "264 0.7003551125526428\n",
      "265 0.6734498739242554\n",
      "266 0.6475694179534912\n",
      "267 0.6227059364318848\n",
      "268 0.5987783670425415\n",
      "269 0.5757109522819519\n",
      "270 0.5537111759185791\n",
      "271 0.5325450897216797\n",
      "272 0.5121387243270874\n",
      "273 0.4925571084022522\n",
      "274 0.4737066626548767\n",
      "275 0.4556036591529846\n",
      "276 0.43824347853660583\n",
      "277 0.421458899974823\n",
      "278 0.40539121627807617\n",
      "279 0.3899710178375244\n",
      "280 0.3751140236854553\n",
      "281 0.36083871126174927\n",
      "282 0.3470882475376129\n",
      "283 0.3339253067970276\n",
      "284 0.3212893009185791\n",
      "285 0.3089905381202698\n",
      "286 0.297292560338974\n",
      "287 0.2860504388809204\n",
      "288 0.2751722037792206\n",
      "289 0.26478415727615356\n",
      "290 0.2547794282436371\n",
      "291 0.24516497552394867\n",
      "292 0.23586951196193695\n",
      "293 0.22693344950675964\n",
      "294 0.21839453279972076\n",
      "295 0.21017111837863922\n",
      "296 0.20223812758922577\n",
      "297 0.1945953071117401\n",
      "298 0.18725794553756714\n",
      "299 0.18019215762615204\n",
      "300 0.17345431447029114\n",
      "301 0.16692809760570526\n",
      "302 0.1606416404247284\n",
      "303 0.154654398560524\n",
      "304 0.1488199532032013\n",
      "305 0.14320802688598633\n",
      "306 0.13784602284431458\n",
      "307 0.13273371756076813\n",
      "308 0.1277513951063156\n",
      "309 0.12299206852912903\n",
      "310 0.11840904504060745\n",
      "311 0.11397232860326767\n",
      "312 0.10968092828989029\n",
      "313 0.10561880469322205\n",
      "314 0.10168273746967316\n",
      "315 0.09788519889116287\n",
      "316 0.09425787627696991\n",
      "317 0.09072566032409668\n",
      "318 0.08733656257390976\n",
      "319 0.08412385731935501\n",
      "320 0.08097729086875916\n",
      "321 0.07796631753444672\n",
      "322 0.07506649941205978\n",
      "323 0.07229910045862198\n",
      "324 0.06964567303657532\n",
      "325 0.06705566495656967\n",
      "326 0.06458687037229538\n",
      "327 0.062160998582839966\n",
      "328 0.05989030748605728\n",
      "329 0.05768768861889839\n",
      "330 0.05556628108024597\n",
      "331 0.0535118542611599\n",
      "332 0.05153364688158035\n",
      "333 0.049623988568782806\n",
      "334 0.04780175909399986\n",
      "335 0.0460498109459877\n",
      "336 0.044360361993312836\n",
      "337 0.042730432003736496\n",
      "338 0.041171953082084656\n",
      "339 0.039656028151512146\n",
      "340 0.03819473832845688\n",
      "341 0.03680753335356712\n",
      "342 0.035459235310554504\n",
      "343 0.03416295722126961\n",
      "344 0.03290821239352226\n",
      "345 0.03171497955918312\n",
      "346 0.03055885247886181\n",
      "347 0.029447978362441063\n",
      "348 0.0283825546503067\n",
      "349 0.027353297919034958\n",
      "350 0.02636951394379139\n",
      "351 0.025410082191228867\n",
      "352 0.024503661319613457\n",
      "353 0.023608602583408356\n",
      "354 0.022743912413716316\n",
      "355 0.02192559465765953\n",
      "356 0.021139664575457573\n",
      "357 0.020375821739435196\n",
      "358 0.0196438767015934\n",
      "359 0.018933119252324104\n",
      "360 0.018259704113006592\n",
      "361 0.017605464905500412\n",
      "362 0.01698072999715805\n",
      "363 0.01636574976146221\n",
      "364 0.01579364389181137\n",
      "365 0.01523319911211729\n",
      "366 0.014691639691591263\n",
      "367 0.014163058251142502\n",
      "368 0.013658973388373852\n",
      "369 0.013168975710868835\n",
      "370 0.012706330046057701\n",
      "371 0.012261277996003628\n",
      "372 0.011826271191239357\n",
      "373 0.011409965343773365\n",
      "374 0.01101388968527317\n",
      "375 0.01062762551009655\n",
      "376 0.010256447829306126\n",
      "377 0.009899483062326908\n",
      "378 0.009557800367474556\n",
      "379 0.00922264065593481\n",
      "380 0.008899519219994545\n",
      "381 0.008597239851951599\n",
      "382 0.008299260400235653\n",
      "383 0.008016771636903286\n",
      "384 0.007743687368929386\n",
      "385 0.007478510495275259\n",
      "386 0.007218541577458382\n",
      "387 0.006976895034313202\n",
      "388 0.00673592509701848\n",
      "389 0.0065055410377681255\n",
      "390 0.006282683461904526\n",
      "391 0.0060712420381605625\n",
      "392 0.005862736608833075\n",
      "393 0.005665775388479233\n",
      "394 0.00547547172755003\n",
      "395 0.0052958400920033455\n",
      "396 0.005123479291796684\n",
      "397 0.004953265190124512\n",
      "398 0.004785601515322924\n",
      "399 0.004626566078513861\n",
      "400 0.004474891349673271\n",
      "401 0.004326776601374149\n",
      "402 0.004190121311694384\n",
      "403 0.004054063931107521\n",
      "404 0.003922580275684595\n",
      "405 0.0037953511346131563\n",
      "406 0.0036721741780638695\n",
      "407 0.003552780020982027\n",
      "408 0.003444608533754945\n",
      "409 0.0033328866120427847\n",
      "410 0.003229327965527773\n",
      "411 0.003129625925794244\n",
      "412 0.0030277061741799116\n",
      "413 0.002934811869636178\n",
      "414 0.002844384638592601\n",
      "415 0.0027556559070944786\n",
      "416 0.0026722426991909742\n",
      "417 0.0025884967762976885\n",
      "418 0.002509882440790534\n",
      "419 0.00243337987922132\n",
      "420 0.002361648716032505\n",
      "421 0.0022879918105900288\n",
      "422 0.0022206264548003674\n",
      "423 0.002154342830181122\n",
      "424 0.0020898852963000536\n",
      "425 0.0020305421203374863\n",
      "426 0.0019719835836440325\n",
      "427 0.0019140677759423852\n",
      "428 0.0018576900474727154\n",
      "429 0.0018046873155981302\n",
      "430 0.0017506919102743268\n",
      "431 0.0017034566262736917\n",
      "432 0.0016537006013095379\n",
      "433 0.0016064728843048215\n",
      "434 0.0015618690522387624\n",
      "435 0.0015171694103628397\n",
      "436 0.0014744132058694959\n",
      "437 0.0014324670191854239\n",
      "438 0.0013922593789175153\n",
      "439 0.0013532607117667794\n",
      "440 0.0013170044403523207\n",
      "441 0.0012814041692763567\n",
      "442 0.0012454886455088854\n",
      "443 0.0012120651081204414\n",
      "444 0.0011799746425822377\n",
      "445 0.0011498181847855449\n",
      "446 0.0011202712776139379\n",
      "447 0.0010877627646550536\n",
      "448 0.001060012960806489\n",
      "449 0.0010315130930393934\n",
      "450 0.0010053839068859816\n",
      "451 0.0009811484487727284\n",
      "452 0.0009544681524857879\n",
      "453 0.0009314530761912465\n",
      "454 0.0009082416072487831\n",
      "455 0.0008850705926306546\n",
      "456 0.0008619986474514008\n",
      "457 0.0008398879435844719\n",
      "458 0.0008200484444387257\n",
      "459 0.000800697656814009\n",
      "460 0.0007795603014528751\n",
      "461 0.0007609027670696378\n",
      "462 0.0007436336018145084\n",
      "463 0.0007243349100463092\n",
      "464 0.0007072154548950493\n",
      "465 0.0006905968184582889\n",
      "466 0.0006747954175807536\n",
      "467 0.0006580984918400645\n",
      "468 0.0006422734004445374\n",
      "469 0.0006274415063671768\n",
      "470 0.0006119947065599263\n",
      "471 0.0005997340776957572\n",
      "472 0.0005856745410710573\n",
      "473 0.0005730552366003394\n",
      "474 0.00055983621859923\n",
      "475 0.0005470555042847991\n",
      "476 0.0005350935971364379\n",
      "477 0.0005233229603618383\n",
      "478 0.0005120076239109039\n",
      "479 0.0005012722685933113\n",
      "480 0.0004903886001557112\n",
      "481 0.00047911680303514004\n",
      "482 0.00046918398584239185\n",
      "483 0.00045978589332662523\n",
      "484 0.0004511425213422626\n",
      "485 0.00044103359687142074\n",
      "486 0.0004315829719416797\n",
      "487 0.00042302580550312996\n",
      "488 0.00041484221583232284\n",
      "489 0.0004065668908879161\n",
      "490 0.0003987381642218679\n",
      "491 0.00039015302900224924\n",
      "492 0.0003823509905487299\n",
      "493 0.00037535754381679\n",
      "494 0.00036801883834414184\n",
      "495 0.0003611476859077811\n",
      "496 0.00035440147621557117\n",
      "497 0.00034741178387776017\n",
      "498 0.0003409488417673856\n",
      "499 0.0003349235630594194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
