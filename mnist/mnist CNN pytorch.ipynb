{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 69\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, train=True):\n",
    "        '''\n",
    "        Args:\n",
    "            csv_file (string)\n",
    "            transform (callable, optional)\n",
    "        '''\n",
    "        \n",
    "        self.mnist = pd.read_csv(csv_file)\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        if self.train:\n",
    "            img = self.mnist.iloc[idx].values[1:]\n",
    "            img = np.reshape(img, (28,28))\n",
    "            img = img / 255\n",
    "            img = Image.fromarray(img)      \n",
    "            \n",
    "            label = self.mnist.iloc[idx][0]\n",
    "            label = np.array(label)\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "                \n",
    "            sample = {\"label\": torch.from_numpy(label), \"image\":img}\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            img = self.mnist.iloc[idx].values[0:]\n",
    "            img = np.reshape(img, (28,28))\n",
    "            img = img / 255\n",
    "            img = Image.fromarray(img)      \n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "                \n",
    "            sample = {\"image\": img}\n",
    "             \n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'label': torch.from_numpy(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = 0.8\n",
    "batch_size = 64\n",
    "\n",
    "mnist_train_dataset = MnistDataset(\"mnist/train.csv\", transform=transforms.Compose([\n",
    "                                                                                    transforms.RandomRotation(2.8),\n",
    "                                                                                    transforms.ToTensor(),\n",
    "                                                                                    transforms.Normalize((0.130,), (0.3081)),\n",
    "                                                                                     ]))\n",
    "mnist_test_dataset = MnistDataset(\"mnist/test.csv\", train=False, transform=transforms.Compose([\n",
    "                                                                                    transforms.ToTensor(),\n",
    "                                                                                    transforms.Normalize((0.130,), (0.3081)),\n",
    "                                                                                     ]))\n",
    "\n",
    "train_length = int(training_split * len(mnist_train_dataset))\n",
    "validation_length = len(mnist_train_dataset) - train_length\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(mnist_train_dataset, (train_length, validation_length))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs_ = 6\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(15,imgs_))\n",
    "for i in range(imgs_):\n",
    "    ax = plt.subplot(1, imgs_, i+1)\n",
    "    ax.set_title('sample #{}'.format(i))\n",
    "    plt.imshow(np.reshape(mnist_train_dataset[i][\"image\"], (28,28)), cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loader, log_interval=500):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        variables = data[\"image\"].to(device)\n",
    "        target = data[\"label\"].to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(variables)\n",
    "        # Calculate loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validate(loss, accuracy_list, loader):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    \n",
    "    for data in loader:\n",
    "        variables = data[\"image\"].to(device)\n",
    "        target = data[\"label\"].to(device)\n",
    "        \n",
    "        \n",
    "        output = model(variables)\n",
    "        val_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        \n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    loss.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/33600 (0%)]\tLoss: 2.290919\n",
      "Train Epoch: 1 [32000/33600 (95%)]\tLoss: 0.524444\n",
      "\n",
      "Validation set: Average loss: 25.1388, Accuracy: 7447/8400 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/33600 (0%)]\tLoss: 0.526603\n",
      "Train Epoch: 2 [32000/33600 (95%)]\tLoss: 0.539616\n",
      "\n",
      "Validation set: Average loss: 18.9402, Accuracy: 7644/8400 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/33600 (0%)]\tLoss: 0.291765\n",
      "Train Epoch: 3 [32000/33600 (95%)]\tLoss: 0.228504\n",
      "\n",
      "Validation set: Average loss: 15.9646, Accuracy: 7776/8400 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/33600 (0%)]\tLoss: 0.370696\n",
      "Train Epoch: 4 [32000/33600 (95%)]\tLoss: 0.412896\n",
      "\n",
      "Validation set: Average loss: 14.0211, Accuracy: 7839/8400 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/33600 (0%)]\tLoss: 0.282870\n",
      "Train Epoch: 5 [32000/33600 (95%)]\tLoss: 0.225410\n",
      "\n",
      "Validation set: Average loss: 12.6790, Accuracy: 7884/8400 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/33600 (0%)]\tLoss: 0.225772\n",
      "Train Epoch: 6 [32000/33600 (95%)]\tLoss: 0.323807\n",
      "\n",
      "Validation set: Average loss: 11.6120, Accuracy: 7935/8400 (94%)\n",
      "\n",
      "Train Epoch: 7 [0/33600 (0%)]\tLoss: 0.098214\n",
      "Train Epoch: 7 [32000/33600 (95%)]\tLoss: 0.315138\n",
      "\n",
      "Validation set: Average loss: 10.6207, Accuracy: 7974/8400 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/33600 (0%)]\tLoss: 0.240100\n",
      "Train Epoch: 8 [32000/33600 (95%)]\tLoss: 0.177099\n",
      "\n",
      "Validation set: Average loss: 9.9503, Accuracy: 7986/8400 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/33600 (0%)]\tLoss: 0.202191\n",
      "Train Epoch: 9 [32000/33600 (95%)]\tLoss: 0.197852\n",
      "\n",
      "Validation set: Average loss: 9.1689, Accuracy: 8026/8400 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/33600 (0%)]\tLoss: 0.224197\n",
      "Train Epoch: 10 [32000/33600 (95%)]\tLoss: 0.450396\n",
      "\n",
      "Validation set: Average loss: 8.6918, Accuracy: 8055/8400 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/33600 (0%)]\tLoss: 0.115256\n",
      "Train Epoch: 11 [32000/33600 (95%)]\tLoss: 0.158528\n",
      "\n",
      "Validation set: Average loss: 8.0472, Accuracy: 8073/8400 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/33600 (0%)]\tLoss: 0.090122\n",
      "Train Epoch: 12 [32000/33600 (95%)]\tLoss: 0.106166\n",
      "\n",
      "Validation set: Average loss: 7.7482, Accuracy: 8079/8400 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/33600 (0%)]\tLoss: 0.147398\n",
      "Train Epoch: 13 [32000/33600 (95%)]\tLoss: 0.163874\n",
      "\n",
      "Validation set: Average loss: 7.2105, Accuracy: 8114/8400 (97%)\n",
      "\n",
      "Train Epoch: 14 [0/33600 (0%)]\tLoss: 0.130762\n",
      "Train Epoch: 14 [32000/33600 (95%)]\tLoss: 0.163848\n",
      "\n",
      "Validation set: Average loss: 6.8314, Accuracy: 8119/8400 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/33600 (0%)]\tLoss: 0.190124\n",
      "Train Epoch: 15 [32000/33600 (95%)]\tLoss: 0.056067\n",
      "\n",
      "Validation set: Average loss: 6.4297, Accuracy: 8145/8400 (97%)\n",
      "\n",
      "Train Epoch: 16 [0/33600 (0%)]\tLoss: 0.065488\n",
      "Train Epoch: 16 [32000/33600 (95%)]\tLoss: 0.194003\n",
      "\n",
      "Validation set: Average loss: 6.1396, Accuracy: 8149/8400 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/33600 (0%)]\tLoss: 0.228166\n",
      "Train Epoch: 17 [32000/33600 (95%)]\tLoss: 0.082785\n",
      "\n",
      "Validation set: Average loss: 5.9085, Accuracy: 8159/8400 (97%)\n",
      "\n",
      "Train Epoch: 18 [0/33600 (0%)]\tLoss: 0.129864\n",
      "Train Epoch: 18 [32000/33600 (95%)]\tLoss: 0.116817\n",
      "\n",
      "Validation set: Average loss: 5.6066, Accuracy: 8171/8400 (97%)\n",
      "\n",
      "Train Epoch: 19 [0/33600 (0%)]\tLoss: 0.121311\n",
      "Train Epoch: 19 [32000/33600 (95%)]\tLoss: 0.051920\n",
      "\n",
      "Validation set: Average loss: 5.4205, Accuracy: 8182/8400 (97%)\n",
      "\n",
      "Train Epoch: 20 [0/33600 (0%)]\tLoss: 0.122754\n",
      "Train Epoch: 20 [32000/33600 (95%)]\tLoss: 0.328407\n",
      "\n",
      "Validation set: Average loss: 5.1789, Accuracy: 8196/8400 (98%)\n",
      "\n",
      "Train Epoch: 21 [0/33600 (0%)]\tLoss: 0.062809\n",
      "Train Epoch: 21 [32000/33600 (95%)]\tLoss: 0.202759\n",
      "\n",
      "Validation set: Average loss: 5.0269, Accuracy: 8197/8400 (98%)\n",
      "\n",
      "Train Epoch: 22 [0/33600 (0%)]\tLoss: 0.041559\n",
      "Train Epoch: 22 [32000/33600 (95%)]\tLoss: 0.034349\n",
      "\n",
      "Validation set: Average loss: 4.8314, Accuracy: 8207/8400 (98%)\n",
      "\n",
      "Train Epoch: 23 [0/33600 (0%)]\tLoss: 0.110657\n",
      "Train Epoch: 23 [32000/33600 (95%)]\tLoss: 0.052996\n",
      "\n",
      "Validation set: Average loss: 4.7049, Accuracy: 8206/8400 (98%)\n",
      "\n",
      "Train Epoch: 24 [0/33600 (0%)]\tLoss: 0.121963\n",
      "Train Epoch: 24 [32000/33600 (95%)]\tLoss: 0.053823\n",
      "\n",
      "Validation set: Average loss: 4.6020, Accuracy: 8220/8400 (98%)\n",
      "\n",
      "Train Epoch: 25 [0/33600 (0%)]\tLoss: 0.116675\n",
      "Train Epoch: 25 [32000/33600 (95%)]\tLoss: 0.095747\n",
      "\n",
      "Validation set: Average loss: 4.4550, Accuracy: 8231/8400 (98%)\n",
      "\n",
      "Train Epoch: 26 [0/33600 (0%)]\tLoss: 0.154088\n",
      "Train Epoch: 26 [32000/33600 (95%)]\tLoss: 0.076345\n",
      "\n",
      "Validation set: Average loss: 4.3219, Accuracy: 8232/8400 (98%)\n",
      "\n",
      "Train Epoch: 27 [0/33600 (0%)]\tLoss: 0.025992\n",
      "Train Epoch: 27 [32000/33600 (95%)]\tLoss: 0.056150\n",
      "\n",
      "Validation set: Average loss: 4.2569, Accuracy: 8230/8400 (98%)\n",
      "\n",
      "Train Epoch: 28 [0/33600 (0%)]\tLoss: 0.034000\n",
      "Train Epoch: 28 [32000/33600 (95%)]\tLoss: 0.138331\n",
      "\n",
      "Validation set: Average loss: 4.1653, Accuracy: 8229/8400 (98%)\n",
      "\n",
      "Train Epoch: 29 [0/33600 (0%)]\tLoss: 0.138826\n",
      "Train Epoch: 29 [32000/33600 (95%)]\tLoss: 0.043364\n",
      "\n",
      "Validation set: Average loss: 4.0173, Accuracy: 8240/8400 (98%)\n",
      "\n",
      "Train Epoch: 30 [0/33600 (0%)]\tLoss: 0.073842\n",
      "Train Epoch: 30 [32000/33600 (95%)]\tLoss: 0.132493\n",
      "\n",
      "Validation set: Average loss: 3.9493, Accuracy: 8245/8400 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "loss_, acc_ = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader)\n",
    "    validate(loss_, acc_ , validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output_list = []\n",
    "for idx, data in enumerate(test_loader):\n",
    "    output = model(data[\"image\"].float().to(device))\n",
    "    output_list.append([idx+1, (output.data.max(1)[1].cpu().numpy().tolist()[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_list) == len(mnist_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "pd.DataFrame(output_list, columns=[\"ImageId\", \"Label\"]).to_csv(\"predictions_cnn.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgan-colorization",
   "language": "python",
   "name": "cgan-colorization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
